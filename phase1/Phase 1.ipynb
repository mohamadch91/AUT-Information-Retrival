{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19287de",
   "metadata": {},
   "source": [
    "# Information retrival project phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd227b",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7b7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "from __future__ import unicode_literals\n",
    "# from parsivar import *\n",
    "import json\n",
    "import string\n",
    "from hazm import stopwords_list\n",
    "from copy import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c93a7",
   "metadata": {},
   "source": [
    "## Reading Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4059df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"IR_data_news_12k.json\")\n",
    "data=json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3dbd23",
   "metadata": {},
   "source": [
    "## create empty arrays to store contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cded74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_content=[]\n",
    "data_url=[]\n",
    "data_title=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57310754",
   "metadata": {},
   "source": [
    "## store content in arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "580d6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    index=str(i)\n",
    "    data_content.append(data[\"\"+index][\"content\"])\n",
    "    data_url.append(data[\"\"+index][\"url\"])\n",
    "    data_title.append(data[\"\"+index][\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da73b1aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#comment after complete\n",
    "# data_content=data_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95133196",
   "metadata": {},
   "source": [
    "## define new array for tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8eea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokens=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c7107",
   "metadata": {},
   "source": [
    "# preproccesing start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c1ea4",
   "metadata": {},
   "source": [
    "## normalize contents\n",
    "tokenize and stemmer and lematize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "266fb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "stop_words=set(stopwords_list())\n",
    "stop_words.add('\\u200cو')\n",
    "stop_words.add('\\u200cو')\n",
    "stop_words.add('و\\u200c')\n",
    "lemmatizer = Lemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790a9cd",
   "metadata": {},
   "source": [
    "## define punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3709c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations=string.punctuation \n",
    "temp_list=set()\n",
    "for i in punctuations:\n",
    "    temp_list.add(i)\n",
    "temp_list.add('،')\n",
    "temp_list.add('.')\n",
    "temp_list.add(':')\n",
    "punctuations=temp_list.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa94dd",
   "metadata": {},
   "source": [
    "### loop over te content and preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d6005",
   "metadata": {},
   "source": [
    "## counting for reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92e2554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=0\n",
    "tokens_num=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75af7c9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range (len(data_content)):\n",
    "    data_content[j]=normalizer.normalize(data_content[j])\n",
    "    new_contents=word_tokenize(data_content[j])\n",
    "    tokens_num.append(math.log(len(new_contents)))\n",
    "    word_count+=len(new_contents)\n",
    "    temp_contents=copy(new_contents)\n",
    "    for k in range(len(temp_contents)):\n",
    "        if(temp_contents[k] in stop_words):\n",
    "            new_contents.remove(temp_contents[k])    \n",
    "        if(temp_contents[k] in punctuations):\n",
    "            new_contents.remove(temp_contents[k])\n",
    "    for k in range(len(new_contents)):\n",
    "        new_contents[k]=lemmatizer.lemmatize(new_contents[k])\n",
    "#         new_contents[k]=stemmer.stem(new_contents[k])\n",
    "\n",
    "    data_tokens.append(new_contents)            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013195f",
   "metadata": {},
   "source": [
    "### indexing words \n",
    "making words indexing in contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6dfdf3",
   "metadata": {},
   "source": [
    "empty set for tokens index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0981076",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_token=dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338e7cd",
   "metadata": {},
   "source": [
    "## iterate over content and tokens \n",
    "find tokens index in each content and <br>\n",
    "find tokens count in each content<br>\n",
    "calculate tokens number in contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a203436a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data_tokens)):\n",
    "    for j in range(len(data_tokens[i])):\n",
    "        if(data_tokens[i][j] in final_token):\n",
    "            temp_string=final_token[data_tokens[i][j]]\n",
    "            last_content=temp_string.split(\"|\")\n",
    "            no=int(last_content[len(last_content)-1].split(\":\")[0].split(\"c\")[1])\n",
    "            if(no==i):\n",
    "                count=last_content[len(last_content)-1].split(\":\")\n",
    "                temp_count=int(count[len(count)-1].split(\"_\")[1])\n",
    "                temp_count+=1\n",
    "                new_string=\"\"\n",
    "                for s in range(len(last_content)-1):\n",
    "                    new_string+=last_content[s]+\"|\"\n",
    "#                 print(last_content[len(last_content)-1])\n",
    "                last_item=last_content[len(last_content)-1].split(\"_\")[0]+\",\"+str(j)+\"_\"+str(temp_count)+\"_\"\n",
    "                new_string+=last_item\n",
    "                final_token[data_tokens[i][j]]=new_string\n",
    "            else: \n",
    "                final_token[data_tokens[i][j]]+=\"|\"+\"c\"+str(i)+\":\"+str(j)+\"_1_\"\n",
    "        else:\n",
    "            final_token[data_tokens[i][j]]=\"|\"+\"c\"+str(i)+\":\"+str(j)+\"_1_\"\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24177a5",
   "metadata": {},
   "source": [
    "## counting tokens for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3229fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4b981e7",
   "metadata": {},
   "source": [
    "# User input Query\n",
    "get input from user to proccess query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c827a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please enter your input :  \"معاملات مشکوک\"\n"
     ]
    }
   ],
   "source": [
    "query=input(\"please enter your input : \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad3187",
   "metadata": {},
   "source": [
    "##### procces input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab6c7f",
   "metadata": {},
   "source": [
    "###### define list for searched wirds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffda6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d98ec",
   "metadata": {},
   "source": [
    "#### define string for handle double quete (\" \")\n",
    "<br>\n",
    "define boolean for handle !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78314a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.split(\" \")\n",
    "temp_string=\"\"\n",
    "forbidden=False\n",
    "query_tokens=set()\n",
    "forbiddens=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da6dd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in query.split(\" \"):\n",
    "    if(word==''):\n",
    "        continue\n",
    "    if(word[0]=='\"'):\n",
    "        temp_string+=word[1:]+\" \"\n",
    "        continue\n",
    "    if(temp_string!=\"\"):\n",
    "        if(word[len(word)-1]=='\"'):\n",
    "            temp_string+=word[:len(word)-1]\n",
    "            query_tokens.add(temp_string)\n",
    "            temp_string=\"\"\n",
    "        else:\n",
    "            temp_string+=word+\" \"\n",
    "        continue\n",
    "    if(word==\"!\"):\n",
    "        forbidden=True\n",
    "        continue\n",
    "    if(forbidden):\n",
    "        forbiddens.add(word)\n",
    "        forbidden=False\n",
    "        continue\n",
    "    query_tokens.add(word)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebbfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ba325",
   "metadata": {},
   "source": [
    "### finding query terms in tokens\n",
    "search in dic <br>\n",
    "and get token indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "410b19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dic=[]\n",
    "forbidden_dic=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa0553cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in query_tokens:\n",
    "        if(len(word.split(\" \"))==1):\n",
    "            if(word in final_token and word not in stop_words):\n",
    "                query_dic.append(final_token[word])\n",
    "        else:\n",
    "            qute=[]\n",
    "            words=word.split(\" \")\n",
    "            for x in words:\n",
    "                if(x in final_token and x not in stop_words):\n",
    "                    qute.append(final_token[x])      \n",
    "            query_dic.append(qute)  \n",
    "for k in forbiddens:\n",
    "    if(k in final_token):\n",
    "        forbidden_dic.append(final_token[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae8adb",
   "metadata": {},
   "source": [
    "#### searching docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8d8f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_point={}\n",
    "for address in query_dic:\n",
    "        if(type(address) is list):\n",
    "            intersection=[]\n",
    "            flag=False\n",
    "            for x in address:\n",
    "                if(len(intersection)==0):\n",
    "                    intersection.append(x)\n",
    "                else:\n",
    "                    intersection_dic={}\n",
    "                    x_dic={}\n",
    "                    answer={}\n",
    "                    inter_doc=intersection[0].split(\"|\")\n",
    "                    inter_doc=inter_doc[1:]\n",
    "                    \n",
    "                    for j in inter_doc:\n",
    "                        docno=j.split(\":\")[0]\n",
    "                        indexes=j.split(\":\")[1]\n",
    "                        intersection_dic[docno]=indexes\n",
    "                    x_doc=x.split(\"|\")\n",
    "                    x_doc=x_doc[1:]\n",
    "                    for j in x_doc:\n",
    "                        x_docno=j.split(\":\")[0]\n",
    "                        x_index=j.split(\":\")[1]\n",
    "                        x_dic[x_docno]=x_index\n",
    "                    for key in intersection_dic.keys():\n",
    "                        if(key in x_dic):\n",
    "                            inter_index=intersection_dic[key].split(\"_\")[0].split(\",\")\n",
    "                            x_index=x_dic[key].split(\"_\")[0].split(\",\")\n",
    "                            for s in inter_index:\n",
    "                                for m in x_index:\n",
    "                                    index_no_inter=int(s)\n",
    "                                    index_no_x=int(m)\n",
    "                                    dif=index_no_x-index_no_inter\n",
    "                                    if(dif==1):\n",
    "                                        if(key in answer):\n",
    "                                            ans_index=answer[key]\n",
    "                                            ans_count=int(ans_index.split(\"_\")[1])\n",
    "                                            ans_indexes=ans_index.split(\"_\")[0]\n",
    "                                            ans_indexes+=\",\"+str(m)\n",
    "                                            ans_count+=1\n",
    "                                            temp_ans=ans_indexes+\"_\"+str(ans_count)+\"_\"\n",
    "                                            answer[key]=temp_ans\n",
    "                                        else:\n",
    "                                            answer[key]=str(m)+\"_1_\"\n",
    "                    temp_s=\"\"     \n",
    "#                     print(answer)\n",
    "                    for key in answer.keys():\n",
    "                        temp_s+=\"|\"+key+\":\"+answer[key]\n",
    "                    answer.clear()\n",
    "                    flag=True\n",
    "                    intersection.clear()\n",
    "                    intersection.append(temp_s)   \n",
    "            if(flag):        \n",
    "                docs=intersection[0].split(\"|\")\n",
    "                for x in docs:\n",
    "                    if(x!=''):\n",
    "                        docid=x.split(\":\")[0].split(\"c\")[1]\n",
    "                        count=x.split(\":\")[1].split(\"_\")\n",
    "                        count=int(count[len(count)-2])\n",
    "                        if(docid in docs_point):\n",
    "                            docs_point[docid]+=count*100\n",
    "                        else:\n",
    "                             docs_point[docid]=count*100\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ff64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for address in query_dic:\n",
    "        if(type(address) is not list):\n",
    "            docs=address.split(\"|\")\n",
    "            for x in docs:\n",
    "                if(x!=''):\n",
    "                    docid=x.split(\":\")[0].split(\"c\")[1]\n",
    "                    count=x.split(\":\")[1].split(\"_\")\n",
    "                    count=int(count[len(count)-2])\n",
    "                    if(docid in docs_point):\n",
    "                        docs_point[docid]+=count\n",
    "                    else:\n",
    "                         docs_point[docid]=count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d6267",
   "metadata": {},
   "source": [
    "### check ! for remove from answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a4e5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for address in forbidden_dic:\n",
    "    doc_id=address.split(\"|\")\n",
    "    for x in doc_id:\n",
    "        if(x!=''):\n",
    "            docid=x.split(\":\")[0].split(\"c\")[1]\n",
    "            if(docid in docs_point):\n",
    "                docs_point.pop(docid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725cece",
   "metadata": {},
   "source": [
    "### print answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f96b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer 1: \n",
      "title: آیت‌الله رئیسی: مبارزه با قاچاق مواد مخدر تشدید شود/ تاکید بر جدی‌گرفتن جمع‌آوری معتادان متجاهر\n",
      "url: https://www.farsnews.ir/news/14000918000702/آیت‌الله-رئیسی-مبارزه-با-قاچاق-مواد-مخدر-تشدید-شود-تاکید-بر\n",
      "10006\n",
      "\n",
      "به گزارش گروه سیاسی خبرگزاری فارس، آیت‌الله سید ابراهیم رئیسی عصر پنج‌شنبه در جلسه ستاد مبارزه با مواد مخدر ضمن گرامیداشت یاد شهدا و جانبازان این عرصه و قدردانی از مرزبانان کشور گفت: تقدیم ۳ هزار و ۸۰۰ شهید و ۱۲ هزار جانباز، نشان ‌دهنده عزم راسخ جمهوری اسلامی ایران برای مبارزه با قاچاقچیان مواد مخدر است. رئیس جمهور مواد مخدر را یکی از تهدیدهای مهم منطقه‌ای و جهانی دانست و با تاکید برلزوم همکاری‌های بین المللی و فعال شدن دیپلماسی مبارزه با مواد مخدر خاطرنشان کرد: باید از سازمان‌های بین‌المللی و کشورهای جهان بپرسیم چقدر به وظیفه خود در مبارزه با مواد مخدر عمل کردند؟، زیرا مساله مبارزه با مواد مخدر موضوعی جهانی است و کشورها باید در سیاست‌های خود در این زمینه بازنگری کنند. رئیسی افزود: زمانی که مواد مخدر را امحاء می‌کنیم سازمان ملل متحد جایزه‌ای برای ایران می‌فرستد در حالی که واقعا تنها فرستادن جایزه کافی نیست؛ زیرا اگر ایران جلوی ترانزیت مواد مخدر را نگیرد این موضوع تهدید و بحرانی برای اروپا خواهد شد. رئیس جمهور درباره کشت مواد مخدر در افغانستان گفت: در دو دهه حضور ناتو و آمریکا در این کشور، تولید مواد مخدر در افغانستان افزایش یافت و تولید مواد مخدر سنتی به صنعتی تبدیل شد که یک تهدید برای جهان است. آیت‌الله رئیسی مهم‌ترین آسیب اجتماعی کشور را موضوع مواد مخدر برشمرد و گفت: ضرورت دارد اقداماتی هدفمند، برنامه‌ریزی شده و با استفاده از تمام ظرفیت‌ها و بسیج امکانات برای مبارزه با مواد مخدر انجام شود. رئیس جمهور مبارزه با مواد مخدر را یک کار تخصصی دانست و تاکید کرد: باید اقدامات اطلاعاتی، قضایی و انتظامی برای مبارزه با قاچاق مواد مخدر تشدید شود تا شاهد کاهش قاچاق و مصرف مواد مخدر باشیم. وی همچنین با تاکید بر اینکه باید از ورود پول‌های ناپاک حاصل از تولید، خرید و فروش مواد مخدر در شبکه پولی و مالی کشور جلوگیری شود، گفت: ضربه زدن به بنیان‌های اقتصادی شبکه‌های قاچاق مواد مخدر یک وظیفه است و حتما باید با معاملات مشکوک و پولشویی حاصل از فروش مواد مخدر مبارزه شود. آیت‌الله رئیسی گفت: شناسایی و برخورد با شبکه‌های توزیع خرد و کلان مواد مخدر از اولویت‌های مبارزه است و حتما جمع‌آوری معتادان متجاهر در دستور کار قرار گیرد وتا رسیدن به نتیجه پیگیری شود. رئیس جمهور با اشاره به اهمیت حرفه‌آموزی و اشتغال معتادان بازپروری شده گفت: ممکن است بازار کار به سهولت افراد با سوء پیشینه‌دار را نپذیرند لذا باید مراکز و امکاناتی برای اشتغال معتادان بازپروری شده ایجاد شود. رئیسی با تاکید به همه استانداران، فرمانداران و بخشداران برای توجه جدی به امر مبارزه با مواد مخدر تصریح کرد: حرکت فرهنگی برای مبارزه با مواد مخدر مهم است و باید آگاهی و حساسیت جوانان و خانواده‌ها در این زمینه افزایش یابد تا از آفات مواد مخدر در امان بمانند. رئیس جمهور حضور و مشارکت فعال دستگاه‌های دخیل در مبارزه با مواد مخدر در ستاد و جلسات تصمیم‌گیری را خواستار شد و گفت: رصد عملیات و اقدامات در مبارزه با مواد مخدر که بیشترین شمار زندانیان کشور را به خود اختصاص داده ضروری و هماهنگی و هم‌پوشانی دستگاه‌ها در این زمینه مهم و تاثیرگذار است. انتهای پیام/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data_content=[]\n",
    "# data_url=[]\n",
    "# data_title=[]\n",
    "final_answer=[]\n",
    "if(len(docs_point)==0):\n",
    "    print(\"no answer found for your query\")  \n",
    "else:\n",
    "    if(len(docs_point)>=5):\n",
    "        for i in range(5):\n",
    "            maximum=0\n",
    "            max_id=\"\"\n",
    "            for docid in docs_point.keys():\n",
    "                if(docs_point[docid]>maximum):\n",
    "                    maximum=docs_point[docid]\n",
    "                    max_id=docid\n",
    "            final_answer.append(max_id)\n",
    "            docs_point.pop(max_id)\n",
    "    else:\n",
    "         for i in range(len(docs_point)):\n",
    "            maximum=0\n",
    "            max_id=\"\"\n",
    "            for docid in docs_point.keys():\n",
    "                if(docs_point[docid]>maximum):\n",
    "                    maximum=docs_point[docid]\n",
    "                    max_id=docid\n",
    "            final_answer.append(max_id)\n",
    "            docs_point.pop(max_id)\n",
    "    count=0    \n",
    "    for k in final_answer:\n",
    "        docid=int(k)\n",
    "        print(\"answer \"+str(count+1)+\": \")\n",
    "        print(\"title: \"+data_title[docid])\n",
    "        print(\"url: \"+data_url[docid])\n",
    "        print(docid)\n",
    "        print(data_content[docid])\n",
    "        count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014c3dc",
   "metadata": {},
   "source": [
    "#### calculate terms number in contents for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccfda155",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_count={}\n",
    "for j in final_token.keys():\n",
    "    count=0\n",
    "    string=final_token[j]\n",
    "    for x in string.split(\"|\")[1:]:\n",
    "        num=int(x.split(\"_\")[1])\n",
    "        count+=num\n",
    "    tokens_count[j]=count\n",
    "z=list(range(1,len(tokens_count)+1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f20894",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_array=[]\n",
    "\n",
    "for j in range(len(tokens_count)):\n",
    "    maximum=0\n",
    "    selected=\"\"\n",
    "    for x in tokens_count.keys():\n",
    "        if(tokens_count[x]>maximum):\n",
    "            maximum=tokens_count[x]\n",
    "            selected=x        \n",
    "    max_array.append(maximum)\n",
    "    tokens_count.pop(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce78be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(z)):\n",
    "    z[l]=math.log(z[l])   \n",
    "for k in range(len(max_array)):\n",
    "    max_array[k]=math.log(max_array[k])\n",
    "  \n",
    "plt.plot(z,max_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcee105",
   "metadata": {},
   "source": [
    "## counting tokens of each content for report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b643d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zz=list(range(1,len(tokens_num)+1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(zz)):\n",
    "    zz[l]=math.log(zz[l])\n",
    "for k in range(1,len(tokens_num)):\n",
    "    tokens_num[k]+=tokens_num[k-1]\n",
    "for k in range(len(tokens_num)):\n",
    "    tokens_num[k]=math.log((tokens_num[k]))\n",
    "    \n",
    "plt.plot(zz,tokens_num)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea99400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210ca3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e78b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
