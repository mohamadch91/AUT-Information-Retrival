{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7596c4f3",
   "metadata": {},
   "source": [
    "# phase 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bef61",
   "metadata": {},
   "source": [
    "## importing librarie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f043e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "from __future__ import unicode_literals\n",
    "# from parsivar import *\n",
    "import json\n",
    "import string\n",
    "from hazm import stopwords_list\n",
    "from copy import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404464c0",
   "metadata": {},
   "source": [
    "## Reading Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2081995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"IR_data_news_12k.json\")\n",
    "data=json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a2724",
   "metadata": {},
   "source": [
    "## create empty arrays to store contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b9f81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_content=[]\n",
    "data_url=[]\n",
    "data_title=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262eff8",
   "metadata": {},
   "source": [
    "## store content in arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9707e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    index=str(i)\n",
    "    data_content.append(data[\"\"+index][\"content\"])\n",
    "    data_url.append(data[\"\"+index][\"url\"])\n",
    "    data_title.append(data[\"\"+index][\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fceac120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment after complete\n",
    "# data_content=data_content[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388de0bd",
   "metadata": {},
   "source": [
    "## define new array for tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da27f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokens=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93746ce6",
   "metadata": {},
   "source": [
    "# preproccesing start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba12c6a",
   "metadata": {},
   "source": [
    "## normalize contents\n",
    "tokenize and stemmer and lematize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47b20e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "stop_words=set(stopwords_list())\n",
    "stop_words.add('\\u200cو')\n",
    "stop_words.add('\\u200cو')\n",
    "stop_words.add('و\\u200c')\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b284cb",
   "metadata": {},
   "source": [
    "## define punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f57f6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations=string.punctuation \n",
    "temp_list=set()\n",
    "for i in punctuations:\n",
    "    temp_list.add(i)\n",
    "temp_list.add('،')\n",
    "temp_list.add('.')\n",
    "temp_list.add(':')\n",
    "punctuations=temp_list.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b09a08",
   "metadata": {},
   "source": [
    "### loop over te content and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range (len(data_content)):\n",
    "    data_content[j]=normalizer.normalize(data_content[j])\n",
    "    new_contents=word_tokenize(data_content[j])\n",
    "    temp_contents=copy(new_contents)\n",
    "    for k in range(len(temp_contents)):\n",
    "        if(temp_contents[k] in stop_words):\n",
    "            new_contents.remove(temp_contents[k])    \n",
    "        if(temp_contents[k] in punctuations):\n",
    "            new_contents.remove(temp_contents[k])\n",
    "#     for k in range(len(new_contents)):\n",
    "#         new_contents[k]=lemmatizer.lemmatize(new_contents[k])\n",
    "# #         new_contents[k]=stemmer.stem(new_contents[k])\n",
    "\n",
    "    data_tokens.append(new_contents)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16bb27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82867acb",
   "metadata": {},
   "source": [
    "## define function for find Tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650486de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tf(content,word):\n",
    "    count=0\n",
    "    for i in content:\n",
    "        if(word==i):\n",
    "            count+=1\n",
    "      \n",
    "    return math.log(count)+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aacf9e2",
   "metadata": {},
   "source": [
    "### indexing words \n",
    "making words indexing in contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1daa6",
   "metadata": {},
   "source": [
    "#### empty set for tokens index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_token=dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95649445",
   "metadata": {},
   "source": [
    "## find Tf for each word and document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_tokens)):\n",
    "    for j in range(len(data_tokens[i])):\n",
    "        if(data_tokens[i][j] in final_token):\n",
    "            temp_string=final_token[data_tokens[i][j]]\n",
    "            last_content=temp_string.split(\"|\")\n",
    "            no=int(last_content[len(last_content)-1].split(\":\")[0].split(\"c\")[1])\n",
    "            if(no==i):\n",
    "                count=last_content[len(last_content)-1].split(\":\")\n",
    "                temp_count=int(count[len(count)-1].split(\"_\")[1])\n",
    "                temp_count+=1\n",
    "                new_string=\"\"\n",
    "                for s in range(len(last_content)-1):\n",
    "                    new_string+=last_content[s]+\"|\"\n",
    "#                 print(last_content[len(last_content)-1])\n",
    "                last_item=last_content[len(last_content)-1].split(\"_\")[0]+\"_\"+str(temp_count)+\"_\"\n",
    "                new_string+=last_item\n",
    "                final_token[data_tokens[i][j]]=new_string\n",
    "            else: \n",
    "                final_token[data_tokens[i][j]]+=\"|\"+\"c\"+str(i)+\":\"+str(find_tf(data_tokens[i],data_tokens[i][j]))+\"_1_\"\n",
    "        else:\n",
    "            final_token[data_tokens[i][j]]=\"|\"+\"c\"+str(i)+\":\"+str(find_tf(data_tokens[i],data_tokens[i][j]))+\"_1_\"\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c67a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c41ee05",
   "metadata": {},
   "source": [
    "##  calculating IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79010a5b",
   "metadata": {},
   "source": [
    "##### function for calulating IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234efddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(string):\n",
    "    temp_str=len(string.split(\"|\"))-1\n",
    "    return math.log(len(data_tokens)/temp_str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in final_token:\n",
    "    final_token[i]+=\"@\"+str(idf(final_token[i]))+\"@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10648a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92a87863",
   "metadata": {},
   "source": [
    "### make champion list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d4ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_token=dict()\n",
    "for i in final_token:\n",
    "    string=final_token[i].split(\"|\")[1:]\n",
    "    idf=final_token[i].split(\"@\")[1]\n",
    "    new_arr=[]\n",
    "    if(len(string)>=5):\n",
    "        for j in range(5):\n",
    "            maximum=-1\n",
    "            maxstr=\"\"\n",
    "            for k in string:\n",
    "                tf=float(k.split(\":\")[1].split(\"_\")[0])\n",
    "                if(maximum<=tf):\n",
    "                    maximum=tf\n",
    "                    maxstr=k\n",
    "            new_arr.append(maxstr)\n",
    "            string.remove(maxstr)\n",
    "        ans=\"\"\n",
    "        for j in new_arr:\n",
    "            ans+=\"|\"+j\n",
    "        ans+=\"@\"+idf+\"@\"    \n",
    "        temp_token[i]=ans\n",
    "        \n",
    "    else:\n",
    "        temp_token[i]=final_token[i]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9011a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_token=temp_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec8ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14cbb9bc",
   "metadata": {},
   "source": [
    "## make vector for each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c927a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf57e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_tokens)):\n",
    "    vector=[]\n",
    "    sum_tf_idf=0\n",
    "    for j in range(len(data_tokens[i])):\n",
    "        term=data_tokens[i][j]\n",
    "        term_data=final_token[term]\n",
    "        \n",
    "        tf_idf=-1\n",
    "#         print(term_data.split(\"@\")[len(term_data.split(\"@\"))-2]\n",
    "        idf=float(term_data.split(\"@\")[len(term_data.split(\"@\"))-2])\n",
    "        term_data=term_data.split(\"|\")[1:]\n",
    "        for k in term_data:\n",
    "            doc_no=int(k.split(\":\")[0].split(\"c\")[1])\n",
    "            if(i==doc_no):\n",
    "#                 print(k.split(\":\")[1].split(\"_\"))\n",
    "                tf=float(k.split(\":\")[1].split(\"_\")[0])\n",
    "                tf_idf=tf*idf\n",
    "        sum_tf_idf+=(tf_idf*tf_idf)\n",
    "#         print(term)\n",
    "#         print(tf_idf)\n",
    "        vector.append(term+\":\"+str(tf_idf))  \n",
    "    sum_tf_idf=math.sqrt(sum_tf_idf)   \n",
    "    for j in range(len(vector)):\n",
    "        word=vector[j].split(\":\")[0]\n",
    "#         print(vector[j].split(\":\"))\n",
    "        if(len(vector[j].split(\":\"))>2):\n",
    "                tf_idf=float(vector[j].split(\":\")[2])\n",
    "        else:        \n",
    "            tf_idf=float(vector[j].split(\":\")[1])\n",
    "        tf_idf/=sum_tf_idf\n",
    "        vector[j]=word+\":\"+str(tf_idf)\n",
    "        \n",
    "    doc_vector.append(vector)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e027798",
   "metadata": {},
   "source": [
    "### nomalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e1706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1528e04e",
   "metadata": {},
   "source": [
    "# User input Query\n",
    "get input from user to proccess query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=input(\"please enter your input : \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845b313",
   "metadata": {},
   "source": [
    "##### process input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1306e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=query.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0ffe1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector=dict()\n",
    "tf_sum=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fdcee2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|c3722:1.0_1_@7.799917239277697@|c548:1.0_1_|c398:1.0_1_|c332:1.0_1_|c265:1.0_1_@7.799917239277697@'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_token[\"ابادان\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd995d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "صنعت\n",
      "2.8081250329829213\n",
      "نفت\n",
      "2.3514572142999413\n",
      "ابادان\n",
      "7.799917239277697\n"
     ]
    }
   ],
   "source": [
    "for i in query:\n",
    "    tf=find_tf(query,i)\n",
    "    idf=0\n",
    "    if(i in final_token):\n",
    "        print(i)\n",
    "        temp=final_token[i]  \n",
    "        idf=float(temp.split(\"@\")[1])\n",
    "        print(idf)\n",
    "    tf_idf=tf*idf    \n",
    "    tf_sum+=(tf_idf*tf_idf)\n",
    "    query_vector[i]=tf_idf\n",
    "math.sqrt(tf_sum)\n",
    "if(tf_sum!=0):\n",
    "    for k in query_vector:\n",
    "        query_vector[k]/=tf_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b28e4c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'صنعت': 0.03781801883333111,\n",
       " 'نفت': 0.03166791085570168,\n",
       " 'ابادان': 0.10504426034765595}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05a8ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ans=[]\n",
    "docs_ans_no=[]\n",
    "for j in range(len(data_tokens)):\n",
    "        ans=0\n",
    "        doc_no=-1\n",
    "        for i in query_vector:\n",
    "            if i in data_tokens[j]:\n",
    "                counter=1\n",
    "                for k in range(len(doc_vector[j])):\n",
    "                    word=doc_vector[j][k].split(\":\")\n",
    "                    if(i==word[0]):\n",
    "                        if(counter==1):\n",
    "#                             print(word)\n",
    "#                             print(query_vector[i])\n",
    "#                             print(word[1])\n",
    "                            ans+=query_vector[i]*float(word[1])\n",
    "                            doc_no=j\n",
    "                            counter-=1\n",
    "        if(ans!=0):\n",
    "            docs_ans.append(ans)\n",
    "            docs_ans_no.append(doc_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9f75bd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# docs_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf9f6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_ans_no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3e3e",
   "metadata": {},
   "source": [
    "# show answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "528a7d1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer 1: \n",
      "title: مدافع استقلال بازی با نفت آبادان را از دست داد/آماده سازی برای دربی پایتخت\n",
      "url: https://www.farsnews.ir/news/14001219000712/مدافع-استقلال-بازی-با-نفت-آبادان-را-از-دست-داد-آماده-سازی-برای-دربی\n",
      "332\n",
      "\n",
      "به گزارش خبرنگار ورزشی خبرگزاری فارس، محمد دانشگر که در تمرینات گروهی روزهای اخیر تیم استقلال حضور نداشت امروز هم در تمرین این تیم شرکت نکرد تا دیدار تیمش مقابل نفت ابادان را از دست بدهد. این بازیکن از ناحیه مچ پا دچار مصدومیت است و به احتمال فراوان به دربی پایتخت می‌رسد. انتهای پیام/\n",
      "\n",
      "\n",
      "answer 2: \n",
      "title: اعلام ترکیب استقلال برای مصاف با نفت آبادان/ اولین بازی سیلوا،ژستد نیمکت نشین شد\n",
      "url: https://www.farsnews.ir/news/14001220000456/اعلام-ترکیب-استقلال-برای-مصاف-با-نفت-آبادان-اولین-بازی-سیلواژستد\n",
      "265\n",
      "\n",
      "به گزارش خبرنگار ورزشی خبرگزاری فارس، فرهاد مجیدی ۱۱ بازیکن تیمش برای مصاف با نفت ابادان در هفته بیست و دوم مسابقات لیگ برتر را به شرح زیر اعلام کرد: سیدحسین حسینی، صالح حردانی، رافائل سیلوا، عارف غلامی، جعفر سلمانی، مهدی مهدی‌پور، عزیزبک آمانوف، رضا آذری، امیرحسین حسین‌زاده، کوین یامگا و آرمان رمضانی. دیدار دو تیم از ساعت ۲۰ در ورزشگاه آزادی آغاز می‌شود. انتهای پیام/\n",
      "\n",
      "\n",
      "answer 3: \n",
      "title: اعلام زمان و مکان نشست خبری استقلال و نفت آبادان\n",
      "url: https://www.farsnews.ir/news/14001218000723/اعلام-زمان-و-مکان-نشست-خبری-استقلال-و-نفت-آبادان\n",
      "398\n",
      "\n",
      "به گزارش خبرگزاری فارس، باشگاه استقلال زمان نشست خبری تیم‌های استقلال و صنعت نفت ابادان به شرح زیر مشخص شد: کنفرانس صالح مصطفوی، مربی تیم استقلال ساعت ۱۵ کنفرانس علیرضا منصوریان، سرمربی تیم صنعت نفت آبادان ساعت ۱۵:۳۰ فرهاد مجیدی سرمربی استقلال به دلیل دریافت کارت قرمز در مسابقه قبلی، از حضور در این نشست خبری محروم است. نشست فردا (پنجشنبه) به میزبانی باشگاه استقلال و در هتل پارسیان آزادی برگزار می‌شود. انتهای پیام/\n",
      "\n",
      "\n",
      "answer 4: \n",
      "title: گزارش تمرین استقلال| گفت و‌گوی غفوری با مجیدی/ دانشگر تمرین نکرد +عکس\n",
      "url: https://www.farsnews.ir/news/14001216000841/گزارش-تمرین-استقلال|-گفت-و‌گوی-غفوری-با-مجیدی-دانشگر-تمرین-نکرد-عکس\n",
      "548\n",
      "\n",
      "به گزارش خبرنگار ورزشی خبرگزاری فارس، تمرین ریکاوری تیم فوتبال استقلال امروز در زمین مجموعه آرارات در شرایطی برگزار شد که بازیکنان اصلی بازی دیروز به ریکاوری پرداختند. *محمد دانشگر به دلیل مصدومیتی که از ناحیه مچ با آن مواجه شده در محل تمرین تیمش حضور داشت اما با آبی پوشان تمرین نکرد. وی زیر نظر کادر پزشکی تیمش به درمان مچ پای خود پرداخت. *در حاشیه تمرین امروز ووریا غفوری با فرهاد مجیدی به صحبت پرداخت. * ابوالفضل جلالی مدافع استقلال که در بازی روز گذشته در لیست این تیم حضور نداشت امروز زیر نظر مربی بدنساز آبی پوشان تمرین کرد. *تمرین فردای تیم فوتبال استقلال تعطیل است. تیم فوتبال استقلال در دیدار آینده خود به مصاف نفت ابادان می‌رود. انتهای پیام/ \n",
      "\n",
      "\n",
      "answer 5: \n",
      "title: لیگ تنیس روی میز|شکست فراسنج با بازیکن کره‌ای و صدرنشینی رعد پدافند\n",
      "url: https://www.farsnews.ir/news/14001105000148/لیگ-تنیس-روی-میز|شکست-فراسنج-با-بازیکن-کره‌ای-و-صدرنشینی-رعد-پدافند\n",
      "3722\n",
      "\n",
      "به گزارش خبرگزاری فارس، مرحله دوم سی امین دوره رقابت‌های لیگ برتر با حضور ۸ تیم از روز یکشنبه سوم بهمن ماه در آکادمی امیر احتشام زاده تهران در حال برگزاری است. در این مرحله از رقابت‌ها تیم رعد پدافند هوایی اصفهان که در پایان مرحله اول با کسب ۱۰ امتیاز در صدر جدول رده بندی جای گرفت، در مرحله دوم با پیروزی ۴ بر ۱ مقابل دانشگاه آزاد و برد قاطع ۵ بر صفر برابر پالایش نفت ابادان با مجموع ۱۴/۵ امتیاز در رده نخست قرار دارد. فراسنج تهران با ۹ امتیاز حاصل از دور اول راهی دور دوم شد. فراسنج که با جذب بازیکن اهل کره جنوبی به دنبال صدرنشینی و تکرار عنوان قهرمانی سال گذشته خود است، ابتدا در برابر عقاب نهاجا ۵ بر صفر به پیروزی رسید، اما در مصاف با شهرداری کرج با نتیجه ۴ بر ۱ مغلوب شد تا شاگردان امیر بخشی در شهرداری کرج بردی روحیه بخش را بدست آورند. تیم فراسنج تهران با ۱۲ امتیاز در رده دوم و شهرداری کرج با ۱۰/۵ امتیاز در مکان سوم جدول رده بندی قرار دارند. تیم دانشگاه آزاد اسلامی در مصاف با رعد پدافند اصفهان۴ بر ۱ مغلوب شد اما در برابر پادما یدک ۳ بر ۲ به پیروزی دست یافت. دانشگاه آزاد با کسب ۸ امتیاز حاصل از دور اول، موفق شد در مجموع با ۱۰ امتیاز در رده چهارم جدول رده بندی قرار گیرد. پادما یدک اصفهان با ۹/۵ امتیاز، پالایش نفت آبادان با مجموع ۸ امتیاز، عقاب نهاجا با ۳/۵ امتیاز و مقاومت تبریز با ۲ امتیاز در رده‌های پنجم تا هشتم قرار دارند. انتهای پیام/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_answer=[]\n",
    "if(len(docs_ans)==0):\n",
    "    print(\"no answer found for your query\")  \n",
    "else:\n",
    "    if(len(docs_ans)>=5):\n",
    "        for i in range(5):\n",
    "            maximum=-10\n",
    "            max_id=0\n",
    "            for k in range(len(docs_ans)):\n",
    "                if(docs_ans[k]>maximum):\n",
    "                    maximum=docs_ans[k]\n",
    "                    max_id=docs_ans_no[k]       \n",
    "            final_answer.append(max_id)\n",
    "            docs_ans.remove(maximum)\n",
    "            docs_ans_no.remove(max_id)\n",
    "    else:\n",
    "         for i in range(len(docs_ans)):\n",
    "            maximum=0\n",
    "            max_id=0\n",
    "            for k in range(len(docs_ans)):\n",
    "                if(docs_ans[k]>maximum):\n",
    "                    maximum=docs_ans[k]\n",
    "                    max_id=docs_ans_no[k]\n",
    "            final_answer.append(max_id)\n",
    "            docs_ans.remove(maximum)\n",
    "            docs_ans_no.remove(max_id)\n",
    "    count=0    \n",
    "    for k in final_answer:\n",
    "        docid=int(k)\n",
    "        print(\"answer \"+str(count+1)+\": \")\n",
    "        print(\"title: \"+data_title[docid])\n",
    "        print(\"url: \"+data_url[docid])\n",
    "        print(docid)\n",
    "        print(data_content[docid])\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ece264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
